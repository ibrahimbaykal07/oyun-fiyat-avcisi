import json
import requests
from bs4 import BeautifulSoup
import os
import re
from datetime import datetime

# --- AYARLAR ---
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}
FILE_NAME = "subscriptions.json"

def clean_name(name):
    # DipnotlarÄ± temizle
    name = re.sub(r'\[.*?\]', '', name)
    name = re.sub(r'\(.*?\)', '', name)
    return name.strip()

def scrape_pcgw_fast(url, target_col_name):
    """
    HÄ±zlÄ± ve Agresif Tarama (Requests + BS4)
    """
    print(f"   ğŸš€ BaÄŸlanÄ±lÄ±yor -> {url}")
    games = []
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=20)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # TÃ¼m tablolarÄ± bul
        tables = soup.find_all('table', {'class': 'wikitable'})
        print(f"   â„¹ï¸ {len(tables)} tablo bulundu.")
        
        for table in tables:
            # SÃ¼tun baÅŸlÄ±klarÄ±nÄ± bul
            headers = [th.get_text(strip=True).lower() for th in table.find_all('th')]
            
            # Hedef sÃ¼tun kaÃ§Ä±ncÄ± sÄ±rada?
            target_idx = -1
            for i, h in enumerate(headers):
                if target_col_name.lower() in h:
                    target_idx = i
                    break
            
            if target_idx == -1: 
                continue # Bu tabloda aradÄ±ÄŸÄ±mÄ±z sÃ¼tun yok, geÃ§.

            # SatÄ±rlarÄ± gez
            rows = table.find_all('tr')
            for row in rows[1:]:
                cols = row.find_all(['td', 'th'])
                
                # Oyun ismi genelde ilk hÃ¼credir
                if not cols: continue
                
                # SÃ¼tun sayÄ±sÄ± tutuyor mu?
                # Not: Bazen TH kullanÄ±ldÄ±ÄŸÄ± iÃ§in index kayabilir, ama genelde 
                # wiki tablolarÄ±nda tÃ¼m hÃ¼creler TD veya TH olarak sÄ±ralÄ±dÄ±r.
                # Ancak 'Game' sÃ¼tunu TH ise, cells listesinde o da vardÄ±r.
                
                # Basit kontrol: SatÄ±rdaki toplam hÃ¼cre sayÄ±sÄ± hedeften bÃ¼yÃ¼k olmalÄ±
                if len(cols) <= target_idx: continue
                
                # Hedef hÃ¼creye bak (YeÅŸil mi?)
                target_cell = cols[target_idx]
                classes = target_cell.get('class', [])
                style = target_cell.get('style', '')
                text = target_cell.get_text(strip=True).lower()
                
                is_active = False
                if 'table-yes' in classes: is_active = True
                elif 'background' in str(style) and ('#90ff90' in str(style) or 'lightgreen' in str(style)): is_active = True
                elif text == 'available': is_active = True
                
                if is_active:
                    name = clean_name(cols[0].get_text(strip=True))
                    if len(name) > 1:
                        games.append(name)

    except Exception as e:
        print(f"   âŒ Hata: {e}")
        
    unique = sorted(list(set(games)))
    print(f"   âœ… Toplanan: {len(unique)}")
    return unique

def scrape_ubisoft_fast():
    # Ubisoft iÃ§in Ã¶zel basit fonksiyon (Sadece isimleri al)
    print("   ğŸš€ Ubisoft+ TaranÄ±yor...")
    games = []
    try:
        url = "https://www.pcgamingwiki.com/wiki/List_of_Ubisoft%2B_games"
        response = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(response.text, 'html.parser')
        tables = soup.find_all('table', {'class': 'wikitable'})
        
        for table in tables:
            for row in table.find_all('tr')[1:]:
                cols = row.find_all(['td', 'th'])
                if cols:
                    name = clean_name(cols[0].get_text(strip=True))
                    if len(name) > 1: games.append(name)
    except: pass
    print(f"   âœ… Toplanan: {len(games)}")
    return list(set(games))

def load_existing_data():
    if os.path.exists(FILE_NAME):
        try:
            with open(FILE_NAME, 'r', encoding='utf-8') as f:
                return json.load(f)
        except: pass
    return {"Game Pass": [], "EA Play": [], "EA Play Pro": [], "Ubisoft+": []}

def main():
    print("ğŸ¤– --- ROBOT BAÅLATILIYOR (V59 - FAST & FURIOUS) ---")
    
    # 1. Verileri Ã‡ek
    print("\n1ï¸âƒ£ Game Pass...")
    gp = scrape_pcgw_fast("https://www.pcgamingwiki.com/wiki/List_of_PC_Game_Pass_games", "Game Pass for PC")
    
    # KONTROL NOKTASI: EÄŸer Game Pass boÅŸsa iÅŸlemi durdur (HATA VER)
    if len(gp) < 50:
        raise Exception(f"âŒ HATA: Game Pass listesi Ã§ekilemedi! Sadece {len(gp)} oyun bulundu.")

    print("\n2ï¸âƒ£ EA Play (Basic)...")
    ea_play = scrape_pcgw_fast("https://www.pcgamingwiki.com/wiki/List_of_EA_Play_games", "EA App")
    
    print("\n3ï¸âƒ£ EA Play Pro...")
    ea_pro = scrape_pcgw_fast("https://www.pcgamingwiki.com/wiki/List_of_EA_Play_games", "EA Play Pro")
    
    # Manuel Pro Destek
    manual_pro = ["FC 26", "FC 25", "F1 24", "Madden NFL 25", "Star Wars Jedi: Survivor"]
    ea_pro = list(set(ea_pro + manual_pro))

    print("\n4ï¸âƒ£ Ubisoft+...")
    ubi = scrape_ubisoft_fast()

    # Hepsini BirleÅŸtir
    final_data = {
        "Game Pass": gp,
        "EA Play": ea_play,
        "EA Play Pro": ea_pro,
        "Ubisoft+": ubi,
        "_meta": {
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    }
    
    # Kaydet
    with open(FILE_NAME, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=4)
        
    print("\nğŸ‰ BAÅARILI! Dosya yazÄ±ldÄ±.")

if __name__ == "__main__":
    main()
