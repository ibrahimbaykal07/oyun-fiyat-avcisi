import json
import requests
from bs4 import BeautifulSoup
import os
import re

# --- AYARLAR ---
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
FILE_NAME = "subscriptions.json"

def clean_name(name):
    """Oyun ismindeki fazlalÄ±klarÄ± temizler"""
    # Ã–rn: "FIFA 23 (2022)" -> "FIFA 23"
    name = re.sub(r'\s*\(.*?\)\s*', '', name)
    return name.strip()

def fetch_pcgw_table(url, table_index=0):
    """PCGamingWiki'den tablo Ã§eken genel fonksiyon"""
    games = []
    try:
        response = requests.get(url, headers={"User-Agent": USER_AGENT})
        if response.status_code != 200: return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        tables = soup.find_all('table', {'class': 'wikitable'})
        
        if not tables or len(tables) <= table_index: return []
        
        # Ä°stenen tabloyu al
        target_table = tables[table_index]
        rows = target_table.find_all('tr')
        
        for row in rows[1:]: # BaÅŸlÄ±ÄŸÄ± atla
            cols = row.find_all(['td', 'th'])
            if cols:
                # Oyun ismi genelde 1. veya 2. sÃ¼tundadÄ±r
                name = cols[0].get_text(strip=True)
                if name:
                    games.append(clean_name(name))
    except Exception as e:
        print(f"âš ï¸ Hata ({url}): {e}")
    
    return list(set(games))

def scrape_gamepass():
    print("â³ Game Pass listesi Ã§ekiliyor...")
    url = "https://www.pcgamingwiki.com/wiki/List_of_PC_Game_Pass_games"
    # Genelde ilk tablo aktif oyunlardÄ±r
    return fetch_pcgw_table(url, 0)

def scrape_ubisoft():
    print("â³ Ubisoft+ listesi Ã§ekiliyor...")
    url = "https://www.pcgamingwiki.com/wiki/List_of_Ubisoft%2B_games"
    return fetch_pcgw_table(url, 0)

def scrape_ea_play():
    print("â³ EA Play & Pro listesi Ã§ekiliyor...")
    url = "https://www.pcgamingwiki.com/wiki/List_of_EA_Play_games"
    ea_play = []
    ea_pro = []
    
    try:
        response = requests.get(url, headers={"User-Agent": USER_AGENT})
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Sayfadaki tÃ¼m baÅŸlÄ±klarÄ± ve tablolarÄ± sÄ±rayla gez
        # PCGamingWiki'de baÅŸlÄ±k (h2/h3) tablonun hemen Ã¼stÃ¼ndedir
        for header in soup.find_all(['h2', 'h3', 'h4']):
            header_text = header.get_text().lower()
            
            # BaÅŸlÄ±ktan sonraki ilk tabloyu bul
            next_node = header.find_next_sibling()
            while next_node and next_node.name != 'table':
                next_node = next_node.find_next_sibling()
            
            if next_node and next_node.name == 'table':
                rows = next_node.find_all('tr')
                temp_games = []
                for row in rows[1:]:
                    cols = row.find_all(['td', 'th'])
                    if cols:
                        temp_games.append(clean_name(cols[0].get_text(strip=True)))
                
                # Listelere daÄŸÄ±t
                if "pro" in header_text:
                    ea_pro.extend(temp_games)
                elif "play" in header_text and "pro" not in header_text:
                    ea_play.extend(temp_games)
                    
    except Exception as e:
        print(f"âš ï¸ EA HatasÄ±: {e}")

    return list(set(ea_play)), list(set(ea_pro))

def load_existing_data():
    """Eski veriyi yÃ¼kle (Yedek)"""
    if os.path.exists(FILE_NAME):
        try:
            with open(FILE_NAME, 'r', encoding='utf-8') as f:
                return json.load(f)
        except: pass
    return {"Game Pass": [], "EA Play": [], "EA Play Pro": [], "Ubisoft+": []}

def main():
    print("ðŸ¤– Robot BaÅŸlatÄ±lÄ±yor (PCGamingWiki Modu)...")
    
    # 1. Eski veriyi hafÄ±zaya al (GÃ¼venlik)
    old_data = load_existing_data()
    final_data = old_data.copy()
    
    # 2. Game Pass Ã‡ek
    gp = scrape_gamepass()
    if gp: 
        final_data["Game Pass"] = gp
        print(f"âœ… Game Pass: {len(gp)} oyun")
    else:
        print("âš ï¸ Game Pass Ã§ekilemedi, eski liste korunuyor.")

    # 3. Ubisoft+ Ã‡ek
    ubi = scrape_ubisoft()
    if ubi:
        final_data["Ubisoft+"] = ubi
        print(f"âœ… Ubisoft+: {len(ubi)} oyun")
    else:
        print("âš ï¸ Ubisoft+ Ã§ekilemedi, eski liste korunuyor.")

    # 4. EA Play & Pro Ã‡ek
    ea_std, ea_pro = scrape_ea_play()
    if ea_std:
        final_data["EA Play"] = ea_std
        print(f"âœ… EA Play: {len(ea_std)} oyun")
    if ea_pro:
        final_data["EA Play Pro"] = ea_pro
        print(f"âœ… EA Play Pro: {len(ea_pro)} oyun")
    
    # 5. Kaydet
    with open(FILE_NAME, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=4)
        
    print("ðŸŽ‰ Ä°ÅŸlem Tamam! subscriptions.json gÃ¼ncellendi.")

if __name__ == "__main__":
    main()
